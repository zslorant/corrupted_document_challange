{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Corrupted document\" challenge solution - Lorant Zsarnowszky**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from wordfreq import word_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the corrupted text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'c:\\Lori\\MUNKA\\world_quant\\wqp-challenge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = pd.read_csv('corrupted_tokens.txt', sep=\" \", header=None, names=[\"Original\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10257</th>\n",
       "      <td>w#e#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27621</th>\n",
       "      <td>t#e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9675</th>\n",
       "      <td>a#d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>t#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8933</th>\n",
       "      <td>A#d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27463</th>\n",
       "      <td>o#e#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>t#e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7696</th>\n",
       "      <td>f#v#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17266</th>\n",
       "      <td>f#r#i#h#d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25643</th>\n",
       "      <td>t#</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Original\n",
       "10257       w#e#\n",
       "27621        t#e\n",
       "9675         a#d\n",
       "723           t#\n",
       "8933         A#d\n",
       "27463       o#e#\n",
       "2562         t#e\n",
       "7696        f#v#\n",
       "17266  f#r#i#h#d\n",
       "25643         t#"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corr.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Original    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for null values\n",
    "df_corr.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "word_list = words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coding the trie for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "from collections.abc import Set\n",
    "\n",
    "\n",
    "class TrieNode(Set):\n",
    "    \n",
    "    # initialization: adding the elements (tokens) to the set\n",
    "    def __init__(self, iterable=()):\n",
    "        self._children = defaultdict(TrieNode)\n",
    "        self._end = False\n",
    "        for element in iterable:\n",
    "            self.add(element)\n",
    "    \n",
    "    # constructing the trie by adding each item (character/letter) of the element to a node and then moving to the next node\n",
    "    def add(self, element):\n",
    "        node = self\n",
    "        for s in element:\n",
    "            node = node._children[s]\n",
    "        node._end = True\n",
    "    \n",
    "    # function on searching on an element and returning False or True if it exists in the trie; valuable for debugging\n",
    "    def __contains__(self, element):\n",
    "        node = self\n",
    "        for k in element:\n",
    "            if k not in node._children:\n",
    "                return False\n",
    "            node = node._children[k]\n",
    "        return node._end\n",
    "    \n",
    "    # function on searching for the element and getting back with all possible results; valuable for debugging\n",
    "    def search(self, term):\n",
    "        results = set() \n",
    "        element = []    \n",
    "        def _search(m, node, i):\n",
    "            element.append(m)\n",
    "            if i == len(term):\n",
    "                if node._end:\n",
    "                    results.add(''.join(element))\n",
    "            elif term[i] == '#':\n",
    "                for k, child in node._children.items():\n",
    "                    _search(k, child, i + 1)\n",
    "            elif term[i] in node._children:\n",
    "                _search(term[i], node._children[term[i]], i + 1)\n",
    "            element.pop()\n",
    "        _search('', self, 0)\n",
    "        return results\n",
    "    \n",
    "    # function with getting back with the most probable solution\n",
    "    def predict(self, term):\n",
    "        search_result = list(self.search(term))\n",
    "        occurence_list=[]\n",
    "        for element in search_result:\n",
    "            occurence_list.append(word_frequency(element, 'en',wordlist='large'))\n",
    "        \n",
    "        if occurence_list:\n",
    "            max_value_index=occurence_list.index(max(occurence_list))\n",
    "            return search_result[max_value_index]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # required by the set class (could be optimized for avoiding recursion)\n",
    "    def __iter__(self):\n",
    "        return iter(self.search('#'))\n",
    "    \n",
    "    # required by the set class (could be optimized by adding each node a count)\n",
    "    def __len__(self):\n",
    "        return sum(1 for _ in self)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding words to the trie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trie = TrieNode()\n",
    "\n",
    "for word in word_list:\n",
    "    my_trie.add(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the tokens from the training file to the trie\n",
    "\n",
    "df = pd.read_csv('training_tokens.txt', sep=\" \", header=None, names=[\"Original\"])\n",
    "\n",
    "training_text = df[\"Original\"].str.cat(sep=\" \").split(\" \")\n",
    "\n",
    "from collections import defaultdict\n",
    "train_text_word_occurence = defaultdict(int)\n",
    "for word in training_text:\n",
    "    train_text_word_occurence[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in training_text:\n",
    "    my_trie.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and running the trie on the corrupted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = pd.read_csv('corrupted_tokens.txt', sep=\" \", header=None, names=[\"Original\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr[\"predictions\"] = df_corr[\"Original\"].apply(my_trie.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T#a#</td>\n",
       "      <td>That</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n#t#</td>\n",
       "      <td>note</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r#c#l#s</td>\n",
       "      <td>recalls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a#</td>\n",
       "      <td>as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e#p#r#e#c#</td>\n",
       "      <td>experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T#e</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>p#s#e#g#r#</td>\n",
       "      <td>passengers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>w#r#</td>\n",
       "      <td>were</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>s#n#</td>\n",
       "      <td>song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>f#r</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>t#</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>c#m#</td>\n",
       "      <td>come</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>u#</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i#</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>t#e</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Original predictions\n",
       "0         T#a#        That\n",
       "1         n#t#        note\n",
       "2      r#c#l#s     recalls\n",
       "3           a#          as\n",
       "4   e#p#r#e#c#  experience\n",
       "5          T#e         The\n",
       "6   p#s#e#g#r#  passengers\n",
       "7         w#r#        were\n",
       "8         s#n#        song\n",
       "9          f#r         for\n",
       "10          t#          to\n",
       "11        c#m#        come\n",
       "12          u#          up\n",
       "13          i#          in\n",
       "14         t#e         the"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corr.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr[\"predictions\"].to_csv('recovered_tokens.txt', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write-up and disclaimer ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking for solutions**:\n",
    "\n",
    "There were a few solutions which came to my mind before choosing the trie implementation. \n",
    "\n",
    "**Splitting the tokens/words in pairs of two** and then iterating through them would also be a solution but in that case I would need to handle the odd words differently than the even ones and also the solution would not be so robust as it would not work with words having two (or more) wildcards beeing next to each other.\n",
    "\n",
    "Finally I decided to use the **trie** as it seemed the best solution in this case.\n",
    "\n",
    "However my solution is by far not the best. First of all I am sure that I could find a larger vocabulary (I used the one from the nltk) as there were a few words which were not in the vocabulary. Those - as you will see - are empty lines. Of course this could be also improved, just by taking a look at these and figuring them out, however that would probably lead to an overfit model. A better vocabulary would solve this problem. \n",
    "\n",
    "The model could be <u>improved by adding POS (part of speech tagging)</u> in case the words/tokens are in sentence order. Also another (or maybe additional) extension would be by adding a <u>word based n-grams</u> to the model. However the trainig of ngrams would be crutial as it could both overfit or underfit the text. \n",
    "\n",
    "As a self-check I trained the trie on the training tokens and and tried to predict them and reached a 86% accuracy. Of course this is an overfitted model, as the test text is probably not from the same text environment as the training text.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
